{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b79a0afa",
      "metadata": {
        "id": "b79a0afa"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4996f8bb",
      "metadata": {
        "id": "4996f8bb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd;\n",
        "import os;\n",
        "from google.colab import files;\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from math import log\n",
        "import operator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# for mounting google drive to access training data from\n",
        "# filepath: /content/drive/MyDrive/Colab Notebooks\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dRbSn_MsaP8v"
      },
      "id": "dRbSn_MsaP8v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data cleaning"
      ],
      "metadata": {
        "id": "0Dw4mRmXGcrM"
      },
      "id": "0Dw4mRmXGcrM"
    },
    {
      "cell_type": "code",
      "source": [
        "# norm(attr:string)\n",
        "# apply zscore normalisation on field 'attr'\n",
        "\n",
        "# continuous_bin(attr:string, bins:int)\n",
        "# create bins distrubuted by percentile intervals equals to 'bins'. Does not yield specified number of bins\n",
        "# if data is very skewed.\n",
        "def norm(dataSet, attr):\n",
        "    dataSet[attr] = (\n",
        "        (dataSet[attr] - dataSet[attr].mean())/dataSet[attr].std()\n",
        "    )\n",
        "def continuous_bin(dataSet, attr, bins):\n",
        "    intervals = 1/bins\n",
        "    percentiles = dataSet[attr].quantile([i*intervals for i in range(0, bins + 1)]).unique()\n",
        "    bins = list(map(lambda x : float(x), percentiles))\n",
        "\n",
        "\n",
        "    dataSet[attr] = pd.cut(\n",
        "        dataSet[attr], bins=bins,\n",
        "        labels=bins[1:]\n",
        "        )\n"
      ],
      "metadata": {
        "id": "VMMwP7uFod6w"
      },
      "id": "VMMwP7uFod6w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0fbe7c4",
      "metadata": {
        "id": "c0fbe7c4"
      },
      "outputs": [],
      "source": [
        "# Task 1 (1)\n",
        "# importing data into dataframes\n",
        "not_loaded = True\n",
        "while not_loaded:\n",
        "    try:\n",
        "        if \"churn_train.csv\" in os.listdir():\n",
        "            churn_train = pd.read_csv(\"customer_churn_dataset-training-master.csv\")\n",
        "            churn_test = pd.read_csv(\"customer_churn_dataset-testing-master.csv\")\n",
        "            not_loaded = False\n",
        "        else:\n",
        "            churn_train = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/customer_churn_dataset-training-master.csv\")\n",
        "            churn_test = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/customer_churn_dataset-testing-master.csv\")\n",
        "            not_loaded = False\n",
        "    except FileNotFoundError as e:\n",
        "        files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for the non continuous attributes\n",
        "for attr in churn_train.columns:\n",
        "    if len(churn_train[attr].unique()) < 10:\n",
        "        print(f\"{attr}: {churn_train[attr].value_counts()}\")"
      ],
      "metadata": {
        "id": "RAZh1xISjusX"
      },
      "id": "RAZh1xISjusX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gender = {\"Female\":0, \"Male\":1}\n",
        "Sub_type = {\"Standard\":0, \"Basic\":1, \"Premium\":2}\n",
        "# converted df\n",
        "conv_churn_train = churn_train.copy()\n",
        "conv_churn_train[\"Gender\"] = conv_churn_train[\"Gender\"].map(gender)\n",
        "conv_churn_train[\"Subscription Type\"] = conv_churn_train[\"Subscription Type\"].map(Sub_type)"
      ],
      "metadata": {
        "id": "iiPh5kKzHv1l"
      },
      "id": "iiPh5kKzHv1l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task1 (2)\n",
        "missing = conv_churn_train.isnull().any(axis=0).sum()\n",
        "total = len(churn_train.index)\n",
        "# print((missing / total)*100)\n",
        "# 0.002% columns are null! There are 9 records that are empty.\n",
        "# Because the missing data takes up a small portion of the training data I will delete samples with missing data\n",
        "\n",
        "clean_churn_train = conv_churn_train.copy().dropna()\n",
        "filled = clean_churn_train.isnull().any(axis=0).sum()\n",
        "print(f\"number of rows with nan:{missing}\\nnumber of rows with nan after cleaning:{filled}\")"
      ],
      "metadata": {
        "id": "bjJqoDdzBsPD"
      },
      "id": "bjJqoDdzBsPD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-processing"
      ],
      "metadata": {
        "id": "rnvB7viYiWO5"
      },
      "id": "rnvB7viYiWO5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1 (3)\n",
        "norm(clean_churn_train, \"Last Interaction\")\n",
        "variance = clean_churn_train['Last Interaction'].std() ** 2\n",
        "mean = clean_churn_train['Last Interaction'].mean()\n",
        "print(f\"var{variance:.2e}, mean{mean:.2e}\\nApproximately 1 and 0\")"
      ],
      "metadata": {
        "id": "LyyuBntjc9HX"
      },
      "id": "LyyuBntjc9HX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "475341a8"
      },
      "source": [
        "# Task 1 (4)\n",
        "continuous_bin(clean_churn_train, \"Total Spend\", 5)\n",
        "print(\"Number of rows for each 'Total Spend' quantile:\")\n",
        "print(clean_churn_train['Total Spend'].value_counts().sort_index())"
      ],
      "id": "475341a8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1 (5)\n",
        "Onehot = pd.get_dummies(clean_churn_train['Contract Length'], prefix='Contract Length')\n",
        "clean_churn_train = pd.concat([clean_churn_train, Onehot], axis=1)\n",
        "clean_churn_train.drop('Contract Length', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "zyoY0ddPClaF"
      },
      "id": "zyoY0ddPClaF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_churn_train = clean_churn_train.copy() #COPY HERE FOR TESTING"
      ],
      "metadata": {
        "id": "gro9hPnKN1Ul"
      },
      "id": "gro9hPnKN1Ul",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1 (6)\n",
        "# Chose Support calls and Tenure to create secondary attribute Support Call Rate.\n",
        "# Support calls has the highest correlation coefficient according to the matrix\n",
        "# Therefore, by extension Support call rate would likely yield similar results\n",
        "\n",
        "clean_churn_train['Support call rate'] =(\n",
        "    clean_churn_train['Support Calls']/clean_churn_train['Tenure']\n",
        ")"
      ],
      "metadata": {
        "id": "uNItQ9hUaXAt"
      },
      "id": "uNItQ9hUaXAt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13b1e1ef"
      },
      "source": [
        "correlation_matrix = clean_churn_train.corr()"
      ],
      "id": "13b1e1ef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "072c915a"
      },
      "source": [
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='RdYlGn', fmt=\".2f\", linewidths=.5)\n",
        "plt.title('Correlation Matrix of All Attributes (Red: High, Green: Low)')\n",
        "plt.show()"
      ],
      "id": "072c915a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2"
      ],
      "metadata": {
        "id": "ZNwI2lOSi-5I"
      },
      "id": "ZNwI2lOSi-5I"
    },
    {
      "cell_type": "code",
      "source": [
        "#find the continuous columns\n",
        "continuous_columns = []\n",
        "\n",
        "for attr in clean_churn_train.columns:\n",
        "    if len(clean_churn_train[attr].unique()) > 10:\n",
        "        if attr != \"CustomerID\":\n",
        "            continuous_columns.append(attr)\n",
        "            print(f\"{attr}\")"
      ],
      "metadata": {
        "id": "3z4k5BmAjAZC"
      },
      "id": "3z4k5BmAjAZC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2 (1)\n",
        "# most pre-processing steps have already been done in part 1.\n",
        "for attr in continuous_columns:\n",
        "    continuous_bin(clean_churn_train, attr, 10)"
      ],
      "metadata": {
        "id": "Z7zcFJGjlEg3"
      },
      "id": "Z7zcFJGjlEg3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7d9cb27"
      },
      "source": [
        "def calcShannonEnt(dataSet):\n",
        "    numEntries = len(dataSet)\n",
        "    labelCounts = {}\n",
        "    for featVec in dataSet: #the the number of unique elements and their occurance\n",
        "        currentLabel = featVec[-1]\n",
        "        if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0\n",
        "        labelCounts[currentLabel] += 1\n",
        "    shannonEnt = 0.0\n",
        "    for key in labelCounts:\n",
        "        prob = float(labelCounts[key])/numEntries\n",
        "        shannonEnt -= prob * log(prob,2) #log base 2\n",
        "    return shannonEnt\n",
        "\n",
        "def splitDataSet(dataSet, axis, value):\n",
        "    retDataSet = []\n",
        "    for featVec in dataSet:\n",
        "        if featVec[axis] == value:\n",
        "            reducedFeatVec = featVec[:axis]     #chop out axis used for splitting\n",
        "            reducedFeatVec.extend(featVec[axis+1:])\n",
        "            retDataSet.append(reducedFeatVec)\n",
        "    return retDataSet\n",
        "\n",
        "def chooseBestFeatureToSplit(dataSet):\n",
        "    print(dataSet)\n",
        "    # prepruning\n",
        "    if dataSet.shape < 20:\n",
        "        return None\n",
        "    numFeatures = len(dataSet[0]) - 1      #the last column is used for the labels\n",
        "    baseEntropy = calcShannonEnt(dataSet)\n",
        "    bestInfoGain = 0.0; bestFeature = -1\n",
        "    for i in range(numFeatures):        #iterate over all the features\n",
        "        featList = [example[i] for example in dataSet] #create a list of all the examples of this feature\n",
        "        uniqueVals = set(featList)       #get a set of unique values\n",
        "        newEntropy = 0.0\n",
        "        for value in uniqueVals:\n",
        "            subDataSet = splitDataSet(dataSet, i, value)\n",
        "            prob = len(subDataSet)/float(len(dataSet))\n",
        "            newEntropy += prob * calcShannonEnt(subDataSet)\n",
        "        infoGain = baseEntropy - newEntropy     #calculate the info gain; ie reduction in entropy\n",
        "        if (infoGain > bestInfoGain):       #compare this to the best gain so far\n",
        "            bestInfoGain = infoGain         #if better than current best, set to best\n",
        "            bestFeature = i\n",
        "        # prepruning\n",
        "        if bestInfoGain < 0.01:\n",
        "            return None\n",
        "    return bestFeature\n",
        "\n",
        "def majorityCnt(classList):\n",
        "    classCount={}\n",
        "    for vote in classList:\n",
        "        if vote not in classCount.keys(): classCount[vote] = 0\n",
        "        classCount[vote] += 1\n",
        "    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
        "    return sortedClassCount[0][0]\n",
        "\n",
        "def createTree(dataSet,labels):\n",
        "    classList = [example[-1] for example in dataSet]\n",
        "    if classList.count(classList[0]) == len(classList):\n",
        "        return classList[0]#stop splitting when all of the classes are equal\n",
        "    if len(dataSet[0]) == 1: #stop splitting when there are no more features in dataSet\n",
        "        return majorityCnt(classList)\n",
        "    bestFeat = chooseBestFeatureToSplit(dataSet)\n",
        "    bestFeatLabel = labels[bestFeat]\n",
        "    myTree = {bestFeatLabel:{}}\n",
        "    del(labels[bestFeat])\n",
        "    featValues = [example[bestFeat] for example in dataSet]\n",
        "    uniqueVals = set(featValues)\n",
        "    for value in uniqueVals:\n",
        "        subLabels = labels[:]       #copy all of labels, so trees don't mess up existing labels\n",
        "        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)\n",
        "    return myTree\n",
        "\n",
        "def classify(inputTree,featLabels,testVec):\n",
        "    firstStr=list(inputTree.keys())[0]\n",
        "    secondDict=inputTree[firstStr]\n",
        "    featIndex=featLabels.index(firstStr)\n",
        "    for key in secondDict:\n",
        "        if testVec[featIndex]==key:\n",
        "            if type(secondDict[key]).__name__=='dict':\n",
        "                classLabel=classify(secondDict[key],featLabels,testVec)\n",
        "            else:\n",
        "                classLabel=secondDict[key]\n",
        "    return classLabel"
      ],
      "id": "e7d9cb27",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "calcShannonEnt(dataSet):\n",
        "Calculates Shannon entropy of the dataset. Measures impurity/uncertainty in class labels. Higher entropy means more mixed classes.\n",
        "\n",
        "splitDataSet(dataSet, axis, value):\n",
        "Splits dataset by a specific feature (axis) with a given value. Returns subset where feature equals value, removing that feature column.\n",
        "\n",
        "chooseBestFeatureToSplit(dataSet):\n",
        "Finds feature with highest information gain. Compares entropy reduction for all features. Returns index of best splitting feature.\n",
        "\n",
        "majorityCnt(classList):\n",
        "Determines most frequent class in a list. Used when no more splits are possible or for tie-breaking.\n",
        "\n",
        "createTree(dataSet, labels):\n",
        "Recursively builds decision tree using ID3 algorithm. Stops when pure classes or no features remain. Returns tree structure.\n",
        "\n",
        "classify(inputTree, featLabels, testVec):\n",
        "Traverses decision tree to classify a test sample. Follows feature branches based on test values until reaching leaf node."
      ],
      "metadata": {
        "id": "z9PiQwTVHmq0"
      },
      "id": "z9PiQwTVHmq0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose to perform pre-pruning to avoid overfitting during the induction of a decision tree\n",
        "\n",
        "pre-pruning halting tree construction early, it prevents the model from developing too many branches that may merely reflect anomalies, noise, or outliers in the training data\n",
        "\n",
        "within chooseBestFeatureToSplit i set two threshold for halting.\n",
        "When the size of the split dataset is less than 20.\n",
        "When the decrease in entropy\\(variance\\) is less than 0.1."
      ],
      "metadata": {
        "id": "QfZ25Tq0Js9o"
      },
      "id": "QfZ25Tq0Js9o"
    },
    {
      "cell_type": "code",
      "source": [
        "Decision_Tree = createTree(clean_churn_train.values.tolist(), clean_churn_train.columns.tolist())"
      ],
      "metadata": {
        "id": "3q_JTKz5wSfK"
      },
      "id": "3q_JTKz5wSfK",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}