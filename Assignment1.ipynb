{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b79a0afa",
      "metadata": {
        "id": "b79a0afa"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4996f8bb",
      "metadata": {
        "id": "4996f8bb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd;\n",
        "import os;\n",
        "from google.colab import files;\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from math import log\n",
        "import operator\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# for mounting google drive to access training data from\n",
        "# filepath: /content/drive/MyDrive/Colab Notebooks\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dRbSn_MsaP8v"
      },
      "id": "dRbSn_MsaP8v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data cleaning"
      ],
      "metadata": {
        "id": "0Dw4mRmXGcrM"
      },
      "id": "0Dw4mRmXGcrM"
    },
    {
      "cell_type": "code",
      "source": [
        "# norm(attr:string)\n",
        "# apply zscore normalisation on field 'attr'\n",
        "\n",
        "# continuous_bin(attr:string, bins:int)\n",
        "# create bins distrubuted by percentile intervals equals to 'bins'. Does not yield specified number of bins\n",
        "# if data is very skewed.\n",
        "def norm(dataSet, attr):\n",
        "    dataSet[attr] = (\n",
        "        (dataSet[attr] - dataSet[attr].mean())/dataSet[attr].std()\n",
        "    )\n",
        "def continuous_bin(dataSet, attr, bins):\n",
        "    intervals = 1/bins\n",
        "    percentiles = dataSet[attr].quantile([i*intervals for i in range(0, bins + 1)]).unique()\n",
        "    bins = list(map(lambda x : float(x), percentiles))\n",
        "\n",
        "\n",
        "    dataSet[attr] = pd.cut(\n",
        "        dataSet[attr], bins=bins,\n",
        "        labels=bins[1:]\n",
        "        )\n",
        "def create_new_column(dataset):\n",
        "    print('Before create_new_column:', dataset[['Support Calls', 'Tenure']].dtypes)\n",
        "\n",
        "    # Ensure 'Support Calls' and 'Tenure' are numeric types for division\n",
        "    support_calls_numeric = pd.to_numeric(dataset['Support Calls'], errors='coerce')\n",
        "    tenure_numeric = pd.to_numeric(dataset['Tenure'], errors='coerce')\n",
        "\n",
        "    # Initialize 'Support Calls rates' column with NaN, then fill based on conditions\n",
        "    dataset['Support Calls rates'] = float('nan')\n",
        "\n",
        "    # Calculate rate where tenure is positive\n",
        "    dataset.loc[tenure_numeric > 0, 'Support Calls rates'] = (\n",
        "        support_calls_numeric / tenure_numeric\n",
        "    )\n",
        "    # Set rate to 0 where tenure is zero or negative (including NaNs from conversion)\n",
        "    # For NaN tenure, (tenure_numeric <= 0) will be false, keeping the rate as NaN, which is correct.\n",
        "    dataset.loc[tenure_numeric <= 0, 'Support Calls rates'] = 0.0\n",
        "\n",
        "    print('After create_new_column (relevant column dtypes):', dataset[['Support Calls rates', 'Support Calls', 'Tenure']].dtypes)"
      ],
      "metadata": {
        "id": "VMMwP7uFod6w"
      },
      "id": "VMMwP7uFod6w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0fbe7c4",
      "metadata": {
        "id": "c0fbe7c4"
      },
      "outputs": [],
      "source": [
        "# Task 1 (1)\n",
        "# importing data into dataframes\n",
        "not_loaded = True\n",
        "while not_loaded:\n",
        "    try:\n",
        "        if \"churn_train.csv\" in os.listdir():\n",
        "            churn_train = pd.read_csv(\"customer_churn_dataset-training-master.csv\")\n",
        "            churn_test = pd.read_csv(\"customer_churn_dataset-testing-master.csv\")\n",
        "            not_loaded = False\n",
        "        else:\n",
        "            churn_train = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/customer_churn_dataset-training-master.csv\")\n",
        "            churn_test = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/customer_churn_dataset-testing-master.csv\")\n",
        "            not_loaded = False\n",
        "    except FileNotFoundError as e:\n",
        "        files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for the non continuous attributes\n",
        "for attr in churn_train.columns:\n",
        "    if len(churn_train[attr].unique()) < 10:\n",
        "        print(f\"{attr}: {churn_train[attr].value_counts()}\")"
      ],
      "metadata": {
        "id": "RAZh1xISjusX"
      },
      "id": "RAZh1xISjusX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gender = {\"Female\":0, \"Male\":1}\n",
        "Sub_type = {\"Standard\":0, \"Basic\":1, \"Premium\":2}\n",
        "# converted df\n",
        "conv_churn_train = churn_train.copy()\n",
        "conv_churn_train[\"Gender\"] = conv_churn_train[\"Gender\"].map(gender)\n",
        "conv_churn_train[\"Subscription Type\"] = conv_churn_train[\"Subscription Type\"].map(Sub_type)"
      ],
      "metadata": {
        "id": "iiPh5kKzHv1l"
      },
      "id": "iiPh5kKzHv1l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task1 (2)\n",
        "missing = conv_churn_train.isnull().any(axis=0).sum()\n",
        "total = len(churn_train.index)\n",
        "# print((missing / total)*100)\n",
        "# 0.002% columns are null! There are 9 records that are empty.\n",
        "# Because the missing data takes up a small portion of the training data I will delete samples with missing data\n",
        "\n",
        "clean_churn_train = conv_churn_train.copy().dropna()\n",
        "filled = clean_churn_train.isnull().any(axis=0).sum()\n",
        "print(f\"number of rows with nan:{missing}\\nnumber of rows with nan after cleaning:{filled}\")"
      ],
      "metadata": {
        "id": "bjJqoDdzBsPD"
      },
      "id": "bjJqoDdzBsPD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-processing"
      ],
      "metadata": {
        "id": "rnvB7viYiWO5"
      },
      "id": "rnvB7viYiWO5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1 (3)\n",
        "norm(clean_churn_train, \"Last Interaction\")\n",
        "variance = clean_churn_train['Last Interaction'].std() ** 2\n",
        "mean = clean_churn_train['Last Interaction'].mean()\n",
        "print(f\"var{variance:.2e}, mean{mean:.2e}\\nApproximately 1 and 0\")"
      ],
      "metadata": {
        "id": "LyyuBntjc9HX"
      },
      "id": "LyyuBntjc9HX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "475341a8"
      },
      "source": [
        "# Task 1 (4)\n",
        "continuous_bin(clean_churn_train, \"Total Spend\", 5)\n",
        "print(\"Number of rows for each 'Total Spend' quantile:\")\n",
        "print(clean_churn_train['Total Spend'].value_counts().sort_index())"
      ],
      "id": "475341a8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1 (5)\n",
        "Onehot = pd.get_dummies(clean_churn_train['Contract Length'], prefix='Contract Length')\n",
        "clean_churn_train = pd.concat([clean_churn_train, Onehot], axis=1)\n",
        "clean_churn_train.drop('Contract Length', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "zyoY0ddPClaF"
      },
      "id": "zyoY0ddPClaF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1 (6)\n",
        "# Chose Support calls and Tenure to create secondary attribute Support Call Rate.\n",
        "# Support calls has the highest correlation coefficient according to the matrix\n",
        "# Therefore, by extension Support call rate would likely yield similar results\n",
        "create_new_column(clean_churn_train)"
      ],
      "metadata": {
        "id": "uNItQ9hUaXAt"
      },
      "id": "uNItQ9hUaXAt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13b1e1ef"
      },
      "source": [
        "correlation_matrix = clean_churn_train.corr()"
      ],
      "id": "13b1e1ef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "072c915a"
      },
      "source": [
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='RdYlGn', fmt=\".2f\", linewidths=.5)\n",
        "plt.title('Correlation Matrix of All Attributes (Red: High, Green: Low)')\n",
        "plt.show()"
      ],
      "id": "072c915a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2"
      ],
      "metadata": {
        "id": "ZNwI2lOSi-5I"
      },
      "id": "ZNwI2lOSi-5I"
    },
    {
      "cell_type": "code",
      "source": [
        "#find the continuous columns\n",
        "continuous_columns = []\n",
        "\n",
        "for attr in clean_churn_train.columns:\n",
        "    if len(clean_churn_train[attr].unique()) > 10:\n",
        "        if attr != \"CustomerID\":\n",
        "            continuous_columns.append(attr)\n",
        "            print(f\"{attr}\")"
      ],
      "metadata": {
        "id": "3z4k5BmAjAZC"
      },
      "id": "3z4k5BmAjAZC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2 (1)\n",
        "# most pre-processing steps have already been done in part 1.\n",
        "for attr in continuous_columns:\n",
        "    continuous_bin(clean_churn_train, attr, 10)"
      ],
      "metadata": {
        "id": "Z7zcFJGjlEg3"
      },
      "id": "Z7zcFJGjlEg3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7d9cb27"
      },
      "source": [
        "def calcShannonEnt(dataSet):\n",
        "    numEntries = len(dataSet)\n",
        "    labelCounts = {}\n",
        "    for featVec in dataSet: #the the number of unique elements and their occurance\n",
        "        currentLabel = featVec[-1]\n",
        "        if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0\n",
        "        labelCounts[currentLabel] += 1\n",
        "    shannonEnt = 0.0\n",
        "    for key in labelCounts:\n",
        "        prob = float(labelCounts[key])/numEntries\n",
        "        shannonEnt -= prob * log(prob,2) #log base 2\n",
        "    return shannonEnt\n",
        "\n",
        "def splitDataSet(dataSet, axis, value):\n",
        "    retDataSet = []\n",
        "    for featVec in dataSet:\n",
        "        if featVec[axis] == value:\n",
        "            reducedFeatVec = featVec[:axis]     #chop out axis used for splitting\n",
        "            reducedFeatVec.extend(featVec[axis+1:])\n",
        "            retDataSet.append(reducedFeatVec)\n",
        "    return retDataSet\n",
        "\n",
        "def chooseBestFeatureToSplit(dataSet):\n",
        "    # prepruning\n",
        "    if len(dataSet) < 20:\n",
        "        return None\n",
        "    numFeatures = len(dataSet[0]) - 1\n",
        "    baseEntropy = calcShannonEnt(dataSet)\n",
        "    bestInfoGain = 0.0; bestFeature = -1\n",
        "    for i in tqdm(range(numFeatures), desc=\"Choosing best feature\"):\n",
        "        featList = [example[i] for example in dataSet]\n",
        "        uniqueVals = set(featList)\n",
        "        newEntropy = 0.0\n",
        "        for value in uniqueVals:\n",
        "            subDataSet = splitDataSet(dataSet, i, value)\n",
        "            prob = len(subDataSet)/float(len(dataSet))\n",
        "            newEntropy += prob * calcShannonEnt(subDataSet)\n",
        "        infoGain = baseEntropy - newEntropy\n",
        "        if (infoGain > bestInfoGain):\n",
        "            bestInfoGain = infoGain\n",
        "            bestFeature = i\n",
        "        # prepruning\n",
        "        if bestInfoGain < 0.1:\n",
        "            return None\n",
        "    return bestFeature\n",
        "\n",
        "def majorityCnt(classList):\n",
        "    classCount={}\n",
        "    for vote in classList:\n",
        "        if vote not in classCount.keys(): classCount[vote] = 0\n",
        "        classCount[vote] += 1\n",
        "    sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True)\n",
        "    return sortedClassCount[0][0]\n",
        "\n",
        "def createTree(dataSet,labels, max_depth=None, current_depth=10):\n",
        "    classList = [example[-1] for example in dataSet]\n",
        "\n",
        "    if classList.count(classList[0]) == len(classList):\n",
        "        return classList[0]\n",
        "    if len(dataSet[0]) == 1 or (max_depth is not None and current_depth >= max_depth):\n",
        "        return majorityCnt(classList)\n",
        "\n",
        "    bestFeat = chooseBestFeatureToSplit(dataSet)\n",
        "    if bestFeat is None:\n",
        "        return majorityCnt(classList)\n",
        "\n",
        "    bestFeatLabel = labels[bestFeat]\n",
        "    myTree = {bestFeatLabel:{}}\n",
        "\n",
        "    # Create a copy of labels to pass to recursive calls, excluding the current best feature's label\n",
        "    subLabels = labels[:bestFeat] + labels[bestFeat+1:]\n",
        "\n",
        "    featValues = [example[bestFeat] for example in dataSet]\n",
        "    uniqueVals = set(featValues)\n",
        "    for value in tqdm(uniqueVals, desc=f\"Building branch for {bestFeatLabel}\"):\n",
        "        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels, max_depth, current_depth + 1)\n",
        "    return myTree\n",
        "\n",
        "def classify(inputTree,featLabels,testVec):\n",
        "    firstStr=list(inputTree.keys())[0]\n",
        "    secondDict=inputTree[firstStr]\n",
        "    featIndex=featLabels.index(firstStr)\n",
        "    for key in secondDict:\n",
        "        if testVec[featIndex]==key:\n",
        "            if type(secondDict[key]).__name__=='dict':\n",
        "                classLabel=classify(secondDict[key],featLabels,testVec)\n",
        "            else:\n",
        "                classLabel=secondDict[key]\n",
        "    return classLabel"
      ],
      "id": "e7d9cb27",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "calcShannonEnt(dataSet):\n",
        "Calculates Shannon entropy of the dataset. Measures impurity/uncertainty in class labels. Higher entropy means more mixed classes.\n",
        "\n",
        "splitDataSet(dataSet, axis, value):\n",
        "Splits dataset by a specific feature (axis) with a given value. Returns subset where feature equals value, removing that feature column.\n",
        "\n",
        "chooseBestFeatureToSplit(dataSet):\n",
        "Finds feature with highest information gain. Compares entropy reduction for all features. Returns index of best splitting feature.\n",
        "\n",
        "majorityCnt(classList):\n",
        "Determines most frequent class in a list. Used when no more splits are possible or for tie-breaking.\n",
        "\n",
        "createTree(dataSet, labels):\n",
        "Recursively builds decision tree using ID3 algorithm. Stops when pure classes or no features remain. Returns tree structure.\n",
        "\n",
        "classify(inputTree, featLabels, testVec):\n",
        "Traverses decision tree to classify a test sample. Follows feature branches based on test values until reaching leaf node."
      ],
      "metadata": {
        "id": "z9PiQwTVHmq0"
      },
      "id": "z9PiQwTVHmq0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose to perform pre-pruning to avoid overfitting during the induction of a decision tree\n",
        "\n",
        "pre-pruning halting tree construction early, it prevents the model from developing too many branches that may merely reflect anomalies, noise, or outliers in the training data\n",
        "\n",
        "within chooseBestFeatureToSplit i set two threshold for halting.\n",
        "When the size of the split dataset is less than 20.\n",
        "When the decrease in entropy\\(variance\\) is less than 0.1."
      ],
      "metadata": {
        "id": "QfZ25Tq0Js9o"
      },
      "id": "QfZ25Tq0Js9o"
    },
    {
      "cell_type": "code",
      "source": [
        "values = clean_churn_train.values.tolist()\n",
        "columns = clean_churn_train.columns.tolist()"
      ],
      "metadata": {
        "id": "fuj1sRshNrsE"
      },
      "id": "fuj1sRshNrsE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(values))"
      ],
      "metadata": {
        "id": "qGcF-2DYOhbd"
      },
      "id": "qGcF-2DYOhbd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Decision_Tree = createTree(values[:1000], columns)"
      ],
      "metadata": {
        "id": "N07KXRIDuHdC"
      },
      "id": "N07KXRIDuHdC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4996fbb"
      },
      "source": [],
      "id": "4996fbb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35300c01"
      },
      "source": [
        "# Task\n",
        "Preprocess the `churn_test` DataFrame by applying the same cleaning and feature engineering steps as done for the training data: map 'Gender' and 'Subscription Type' using existing dictionaries, drop rows with missing values, create a 'Support call rate' feature, normalize 'Last Interaction' using Z-score, apply continuous binning with 5 bins to 'Total Spend', perform one-hot encoding on 'Contract Length', and apply continuous binning with 10 bins to 'Age', 'Tenure', 'Usage Frequency', 'Support Calls', 'Payment Delay', 'Last Interaction', and 'Support call rate', then display the shape, head, and missing value counts of the resulting `clean_churn_test` DataFrame.\n"
      ],
      "id": "35300c01"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8a34d09"
      },
      "source": [
        "train pre-processsing\n"
      ],
      "id": "a8a34d09"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5646e556"
      },
      "source": [
        "clean_churn_test = churn_test.copy()"
      ],
      "id": "5646e556",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "781489c7"
      },
      "source": [
        "clean_churn_test[\"Gender\"] = clean_churn_test[\"Gender\"].map(gender)\n",
        "clean_churn_test[\"Subscription Type\"] = clean_churn_test[\"Subscription Type\"].map(Sub_type)"
      ],
      "id": "781489c7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26425c24"
      },
      "source": [
        "missing_before = clean_churn_test.isnull().any(axis=1).sum()\n",
        "clean_churn_test.dropna(inplace=True)\n",
        "missing_after = clean_churn_test.isnull().any(axis=1).sum()"
      ],
      "id": "26425c24",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51d91f18"
      },
      "source": [
        "create_new_column(clean_churn_test)"
      ],
      "id": "51d91f18",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21a3600a"
      },
      "source": [
        "norm(clean_churn_test, \"Last Interaction\")"
      ],
      "id": "21a3600a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f672b84"
      },
      "source": [
        "**Reasoning**:\n",
        "To maintain consistency with the preprocessing applied to the training data, the 'Total Spend' column in `clean_churn_test` needs to be binned into 5 continuous intervals using the `continuous_bin` function.\n",
        "\n"
      ],
      "id": "3f672b84"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0fd67ed"
      },
      "source": [
        "continuous_bin(clean_churn_test, \"Total Spend\", 5)"
      ],
      "id": "f0fd67ed",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba5876e6"
      },
      "source": [
        "Onehot_test = pd.get_dummies(clean_churn_test['Contract Length'], prefix='Contract Length')\n",
        "clean_churn_test = pd.concat([clean_churn_test, Onehot_test], axis=1)\n",
        "clean_churn_test.drop('Contract Length', axis=1, inplace=True)"
      ],
      "id": "ba5876e6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fa61de0"
      },
      "source": [
        "for attr in continuous_columns:\n",
        "    continuous_bin(clean_churn_test, attr, 10)"
      ],
      "id": "8fa61de0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "790d3a23"
      },
      "source": [
        "print(\"Shape of clean_churn_test:\", clean_churn_test.shape)\n",
        "print(\"\\nHead of clean_churn_test:\")\n",
        "print(clean_churn_test.head())\n",
        "print(\"\\nMissing values in clean_churn_test:\")\n",
        "print(clean_churn_test.isnull().sum())"
      ],
      "id": "790d3a23",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(clean_churn_test.isnull().head())"
      ],
      "metadata": {
        "id": "R2MTUCEKXQDP"
      },
      "id": "R2MTUCEKXQDP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69652cb6"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous output revealed that `clean_churn_test` still contains missing values in several columns after all preprocessing steps, contradicting the intention to drop all rows with missing values. To fully comply with the task's requirement and ensure a clean dataset for subsequent steps, I need to explicitly drop these remaining rows with missing values, and then re-display the requested information.\n",
        "\n"
      ],
      "id": "69652cb6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "143bbd32"
      },
      "source": [
        "print(f\"Shape of clean_churn_test before final dropna: {clean_churn_test.shape}\")\n",
        "clean_churn_test.dropna(inplace=True)\n",
        "print(f\"Shape of clean_churn_test after final dropna: {clean_churn_test.shape}\")\n",
        "print(\"\\nHead of clean_churn_test after final dropna:\")\n",
        "print(clean_churn_test.head())\n",
        "print(\"\\nMissing values in clean_churn_test after final dropna:\")\n",
        "print(clean_churn_test.isnull().sum())"
      ],
      "id": "143bbd32",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}